{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following I use a simple convolutional neural network to classify apparel from the fashion-MNIST dataset. I'm curious to see how well it works compared to the conventional MNIST dataset!\n",
    "\n",
    "https://github.com/zalandoresearch/fashion-mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data\n",
    "training_set = pd.read_csv('fashionmnist/fashion-mnist_train.csv')\n",
    "test_set = pd.read_csv('fashionmnist/fashion-mnist_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['label', 'pixel1', 'pixel2', 'pixel3', 'pixel4', 'pixel5', 'pixel6',\n",
       "       'pixel7', 'pixel8', 'pixel9',\n",
       "       ...\n",
       "       'pixel775', 'pixel776', 'pixel777', 'pixel778', 'pixel779', 'pixel780',\n",
       "       'pixel781', 'pixel782', 'pixel783', 'pixel784'],\n",
       "      dtype='object', length=785)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For each entry of the dataset, there is a label as well as a value for each pixel in the image (28x28 pixels)\n",
    "# test_set is similar\n",
    "training_set.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a dictionary to help convert the integer labels into what they respresent for future use\n",
    "label_names ={0:'top', 1:'trouser', 2:'pullover', 3:'dress', 4:'coat', 5:'sandal', 6:'shirt', 7:'sneaker', 8:'bag', 9:'ankle boot'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train/valid\n",
    "train_features, valid_features, train_labels, valid_labels = train_test_split(training_set[list(training_set.columns)[1:]], training_set['label'], test_size = .15)\n",
    "# split test set into features and labels \n",
    "test_features = test_set[list(test_set.columns)[1:]]\n",
    "test_labels = test_set['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshaping the features so that the pixel values are arranged according to their place\n",
    "# converting to np arrays for use in model\n",
    "num_col = 28\n",
    "num_row = 28\n",
    "train_features = np.array(train_features).reshape(train_features.shape[0], num_row, num_col, 1)\n",
    "valid_features = np.array(valid_features).reshape(valid_features.shape[0], num_row, num_col, 1)\n",
    "test_features = np.array(test_features).reshape(test_features.shape[0], num_row, num_col, 1)\n",
    "\n",
    "train_labels = np.array(train_labels)\n",
    "valid_labels = np.array(valid_labels)\n",
    "test_labels = np.array(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sneaker\n"
     ]
    }
   ],
   "source": [
    "# we can build images from the data using the pixel values\n",
    "example_image = train_features[0].reshape(28,28)\n",
    "fig, ax = plt.subplots(1, 1, figsize = (7, 7))\n",
    "example = ax.imshow(example_image, cmap = cm.Greys_r, interpolation = 'none')    \n",
    "plt.show()\n",
    "# show the label for the image\n",
    "print(label_names[train_labels[0]])\n",
    "\n",
    "# while the image isn't the best, and this certainly isn't the best way to generate it, it's vaguely clear what the item is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# checking if there's a class imbalance, turns out all classes have about equal representation\n",
    "plt.hist(train_labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some final processing to normalize the pixel values to [0,1], and labels to appropiate categorical format \n",
    "train_features = train_features / 255\n",
    "valid_features = valid_features / 255\n",
    "test_features = test_features / 255\n",
    "\n",
    "train_labels = to_categorical(train_labels)\n",
    "valid_labels = to_categorical(valid_labels)\n",
    "test_labels = to_categorical(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(51000, 10)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done with the initial data processing, moving onto building the model!\n",
    "Tensorflow with keras API is used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Convolution2D, MaxPooling2D, Dropout\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras import initializers\n",
    "from tensorflow.keras import backend as K\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are most of the hyperparameters that are used for tuning (along with some constants such as input shape).\n",
    "Some observations:\n",
    "Dropout probability (prob_drop) has to be carefully set to prevent the model from overfitting too much while still learning well. .4 works pretty well.\n",
    "Smaller pool size tends to slow the learning rate significantly. Simply halving (2,2) seems best. \n",
    "Having 2 convolution layers and 2 fully connected layers has best performance. Similar performance could probably be had using more (and maybe fewer) layers with other hyperparameters changed. \n",
    "32 filters tends to work well for the \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (28,28,1)\n",
    "prob_drop = .3 # usually good around .2 - .4\n",
    "pool_size = (2,2)\n",
    "classes = 10\n",
    "epochs = 20 # usually converges around 10-15\n",
    "batch_size = 100 \n",
    "max_conv_layers = 2 # best 2\n",
    "max_fc_layers = 2 # best 2\n",
    "first_layer_filters = 32 # 32 works well\n",
    "filters = 64 # 32 works well, more doesn't improve results\n",
    "dense_units = 512\n",
    "kernel_size = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Programming\\Anacondaaa\\envs\\TF\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From D:\\Programming\\Anacondaaa\\envs\\TF\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 28, 28, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 14, 14, 64)        18496     \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 12544)             0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 12544)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               6423040   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 6,446,986\n",
      "Trainable params: 6,446,986\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "## using the keras sequential model for the CNN\n",
    "model = Sequential()\n",
    "\n",
    "# input layer/convolutional layer 1\n",
    "\n",
    "# convolutional layers convert the original matrix of values into another matrix representing different parts of the image.\n",
    "# as the model learns, it will get better as converting parts of the image into useful values\n",
    "model.add(Convolution2D(filters = first_layer_filters, kernel_size = kernel_size, strides = 1, activation='relu', \n",
    "                        input_shape=input_shape, padding = 'same'))\n",
    "\n",
    "# pooling helps make the model more efficient by reducing the size of the representation outputted by the Convolution2D portion\n",
    "model.add(MaxPooling2D(pool_size=pool_size, padding = 'same', strides=(2,2)))\n",
    "\n",
    "#lower the dimension before passing to fully connected layers, if this is the last layer\n",
    "if max_conv_layers == 1:\n",
    "    model.add(Flatten())\n",
    "# dropout layers cause random connections to be dropped at each training step. This helps prevent the model from overfitting\n",
    "model.add(Dropout(rate = prob_drop))\n",
    "\n",
    "# add more convolutional layers\n",
    "for conv_layer in range(max_conv_layers - 1):\n",
    "    model.add(Convolution2D(filters = filters, kernel_size = kernel_size, strides = 1, activation = 'relu', padding = 'same'))\n",
    "    #model.add(MaxPooling2D(pool_size = pool_size, padding = 'same', strides=(2,2)))\n",
    "    # for the last convolutional layer, lower the dimension before passing to fully connected layers\n",
    "    if conv_layer == (max_conv_layers - 2):\n",
    "        model.add(Flatten())\n",
    "    model.add(Dropout(rate = prob_drop))\n",
    "    \n",
    "# add some fully connected layers for the end steps of the model\n",
    "for fc_layer in range(max_fc_layers - 1):\n",
    "    # a simple dense neural network\n",
    "    model.add(Dense(dense_units, activation = 'relu'))\n",
    "    # dropout layer to prevent overfitting\n",
    "    model.add(Dropout(rate = prob_drop))\n",
    "\n",
    "# final classification layer (uses softmax to output values for each of the 10 classes)\n",
    "model.add(Dense(classes, activation = 'softmax'))\n",
    "\n",
    "# simple summary of the model\n",
    "model.summary()\n",
    "\n",
    "opt = RMSprop(lr=0.001, rho=0.9)\n",
    "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 51000 samples, validate on 9000 samples\n",
      "WARNING:tensorflow:From D:\\Programming\\Anacondaaa\\envs\\TF\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/20\n",
      "51000/51000 [==============================] - 9s 180us/sample - loss: 0.4769 - acc: 0.8285 - val_loss: 0.3178 - val_acc: 0.8854\n",
      "Epoch 2/20\n",
      "51000/51000 [==============================] - 8s 152us/sample - loss: 0.3063 - acc: 0.8891 - val_loss: 0.2595 - val_acc: 0.9079\n",
      "Epoch 3/20\n",
      "51000/51000 [==============================] - 8s 151us/sample - loss: 0.2659 - acc: 0.9045 - val_loss: 0.2334 - val_acc: 0.9170\n",
      "Epoch 4/20\n",
      "51000/51000 [==============================] - 8s 151us/sample - loss: 0.2451 - acc: 0.9119 - val_loss: 0.2427 - val_acc: 0.9124\n",
      "Epoch 5/20\n",
      "51000/51000 [==============================] - 8s 151us/sample - loss: 0.2277 - acc: 0.9170 - val_loss: 0.2353 - val_acc: 0.9191\n",
      "Epoch 6/20\n",
      "51000/51000 [==============================] - 8s 151us/sample - loss: 0.2205 - acc: 0.9213 - val_loss: 0.2271 - val_acc: 0.9211\n",
      "Epoch 7/20\n",
      "51000/51000 [==============================] - 8s 152us/sample - loss: 0.2107 - acc: 0.9250 - val_loss: 0.2381 - val_acc: 0.9157\n",
      "Epoch 8/20\n",
      "51000/51000 [==============================] - 8s 152us/sample - loss: 0.2033 - acc: 0.9266 - val_loss: 0.2456 - val_acc: 0.9207\n",
      "Epoch 9/20\n",
      "51000/51000 [==============================] - 8s 152us/sample - loss: 0.2007 - acc: 0.9287 - val_loss: 0.2226 - val_acc: 0.9277\n",
      "Epoch 10/20\n",
      "51000/51000 [==============================] - 8s 152us/sample - loss: 0.1982 - acc: 0.9297 - val_loss: 0.2334 - val_acc: 0.9227\n",
      "Epoch 11/20\n",
      "51000/51000 [==============================] - 8s 152us/sample - loss: 0.1981 - acc: 0.9304 - val_loss: 0.2203 - val_acc: 0.9247\n",
      "Epoch 12/20\n",
      "51000/51000 [==============================] - 8s 151us/sample - loss: 0.1983 - acc: 0.9306 - val_loss: 0.2377 - val_acc: 0.9261\n",
      "Epoch 13/20\n",
      "51000/51000 [==============================] - 8s 151us/sample - loss: 0.1940 - acc: 0.9337 - val_loss: 0.2310 - val_acc: 0.9253\n",
      "Epoch 14/20\n",
      "51000/51000 [==============================] - 8s 151us/sample - loss: 0.1940 - acc: 0.9333 - val_loss: 0.2938 - val_acc: 0.9169\n",
      "Epoch 15/20\n",
      "51000/51000 [==============================] - 8s 155us/sample - loss: 0.1936 - acc: 0.9329 - val_loss: 0.2578 - val_acc: 0.9229\n",
      "Epoch 16/20\n",
      "51000/51000 [==============================] - 8s 154us/sample - loss: 0.1927 - acc: 0.9339 - val_loss: 0.2401 - val_acc: 0.9240\n",
      "Epoch 17/20\n",
      "51000/51000 [==============================] - 8s 151us/sample - loss: 0.1926 - acc: 0.9337 - val_loss: 0.2376 - val_acc: 0.9236\n",
      "Epoch 18/20\n",
      "51000/51000 [==============================] - 8s 150us/sample - loss: 0.1921 - acc: 0.9348 - val_loss: 0.3465 - val_acc: 0.9178\n",
      "Epoch 19/20\n",
      "51000/51000 [==============================] - 8s 150us/sample - loss: 0.1877 - acc: 0.9351 - val_loss: 0.2478 - val_acc: 0.9201\n",
      "Epoch 20/20\n",
      "51000/51000 [==============================] - 8s 150us/sample - loss: 0.1916 - acc: 0.9347 - val_loss: 0.3140 - val_acc: 0.9197\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_features, train_labels, epochs = epochs, batch_size = batch_size, validation_data = (valid_features, valid_labels), shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After finding sufficiently good hyperparameters, the model is retrained using all the train and validation data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = np.vstack((train_features, valid_features))\n",
    "labels = np.vstack((train_labels, valid_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_2 (Conv2D)            (None, 28, 28, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 14, 14, 64)        18496     \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 12544)             0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 12544)             0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               6423040   \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 6,446,986\n",
      "Trainable params: 6,446,986\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "## using the keras sequential model for the CNN\n",
    "model = Sequential()\n",
    "\n",
    "# input layer/convolutional layer 1\n",
    "\n",
    "# convolutional layers convert the original matrix of values into another matrix representing different parts of the image.\n",
    "# as the model learns, it will get better as converting parts of the image into useful values\n",
    "model.add(Convolution2D(filters = first_layer_filters, kernel_size = kernel_size, strides = 1, activation='relu', \n",
    "                        input_shape=input_shape, padding = 'same'))\n",
    "\n",
    "# pooling helps make the model more efficient by reducing the size of the representation outputted by the Convolution2D portion\n",
    "model.add(MaxPooling2D(pool_size=pool_size, padding = 'same', strides=(2,2)))\n",
    "\n",
    "#lower the dimension before passing to fully connected layers, if this is the last layer\n",
    "if max_conv_layers == 1:\n",
    "    model.add(Flatten())\n",
    "# dropout layers cause random connections to be dropped at each training step. This helps prevent the model from overfitting\n",
    "model.add(Dropout(rate = prob_drop))\n",
    "\n",
    "# add more convolutional layers\n",
    "for conv_layer in range(max_conv_layers - 1):\n",
    "    model.add(Convolution2D(filters = filters, kernel_size = kernel_size, strides = 1, activation = 'relu', padding = 'same'))\n",
    "    #model.add(MaxPooling2D(pool_size = pool_size, padding = 'same', strides=(2,2)))\n",
    "    # for the last convolutional layer, lower the dimension before passing to fully connected layers\n",
    "    if conv_layer == (max_conv_layers - 2):\n",
    "        model.add(Flatten())\n",
    "    model.add(Dropout(rate = prob_drop))\n",
    "    \n",
    "# add some fully connected layers for the end steps of the model\n",
    "for fc_layer in range(max_fc_layers - 1):\n",
    "    # a simple dense neural network\n",
    "    model.add(Dense(dense_units, activation = 'relu'))\n",
    "    # dropout layer to prevent overfitting\n",
    "    model.add(Dropout(rate = prob_drop))\n",
    "\n",
    "# final classification layer (uses softmax to output values for each of the 10 classes)\n",
    "model.add(Dense(classes, activation = 'softmax'))\n",
    "\n",
    "# simple summary of the model\n",
    "model.summary()\n",
    "\n",
    "opt = RMSprop(lr=0.001, rho=0.9)\n",
    "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "60000/60000 [==============================] - 9s 158us/sample - loss: 0.4497 - acc: 0.8378 - val_loss: 0.3199 - val_acc: 0.8849\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 9s 152us/sample - loss: 0.2944 - acc: 0.8934 - val_loss: 0.2595 - val_acc: 0.9101\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 9s 152us/sample - loss: 0.2557 - acc: 0.9072 - val_loss: 0.2446 - val_acc: 0.9129\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 9s 152us/sample - loss: 0.2373 - acc: 0.9135 - val_loss: 0.2335 - val_acc: 0.9218\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 9s 152us/sample - loss: 0.2240 - acc: 0.9201 - val_loss: 0.2120 - val_acc: 0.9255\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 9s 152us/sample - loss: 0.2180 - acc: 0.9230 - val_loss: 0.2300 - val_acc: 0.9274\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 9s 151us/sample - loss: 0.2153 - acc: 0.9241 - val_loss: 0.2212 - val_acc: 0.9235\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 9s 152us/sample - loss: 0.2095 - acc: 0.9254 - val_loss: 0.2477 - val_acc: 0.9221\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 9s 152us/sample - loss: 0.2063 - acc: 0.9276 - val_loss: 0.2168 - val_acc: 0.9248\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 9s 151us/sample - loss: 0.2042 - acc: 0.9286 - val_loss: 0.2073 - val_acc: 0.9290\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 9s 152us/sample - loss: 0.2030 - acc: 0.9287 - val_loss: 0.2366 - val_acc: 0.9222\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 9s 152us/sample - loss: 0.2006 - acc: 0.9305 - val_loss: 0.3377 - val_acc: 0.9161\n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 9s 151us/sample - loss: 0.2036 - acc: 0.9294 - val_loss: 0.2362 - val_acc: 0.9273\n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 9s 151us/sample - loss: 0.2008 - acc: 0.9310 - val_loss: 0.2136 - val_acc: 0.9304\n",
      "Epoch 15/20\n",
      "60000/60000 [==============================] - 9s 151us/sample - loss: 0.2018 - acc: 0.9306 - val_loss: 0.2344 - val_acc: 0.9238\n",
      "Epoch 16/20\n",
      "60000/60000 [==============================] - 9s 151us/sample - loss: 0.2017 - acc: 0.9309 - val_loss: 0.2814 - val_acc: 0.9224\n",
      "Epoch 17/20\n",
      "60000/60000 [==============================] - 9s 151us/sample - loss: 0.2009 - acc: 0.9314 - val_loss: 0.2486 - val_acc: 0.9266\n",
      "Epoch 18/20\n",
      "60000/60000 [==============================] - 9s 150us/sample - loss: 0.2013 - acc: 0.9310 - val_loss: 0.2260 - val_acc: 0.9253\n",
      "Epoch 19/20\n",
      "60000/60000 [==============================] - 9s 150us/sample - loss: 0.2014 - acc: 0.9311 - val_loss: 0.2850 - val_acc: 0.9185\n",
      "Epoch 20/20\n",
      "60000/60000 [==============================] - 9s 151us/sample - loss: 0.2049 - acc: 0.9312 - val_loss: 0.3204 - val_acc: 0.9158\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(features, labels, epochs = epochs, batch_size = batch_size, validation_data = (test_features, test_labels), shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 74us/sample - loss: 0.3204 - acc: 0.9158\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.32038994684219363, 0.9158]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# final evaluation using the test data\n",
    "model.evaluate(test_features, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted label: top\n",
      "Actual label: top\n"
     ]
    }
   ],
   "source": [
    "# quick example classification\n",
    "example_image = test_features[0].reshape(28,28)\n",
    "fig, ax = plt.subplots(1, 1, figsize = (7, 7))\n",
    "example = ax.imshow(example_image, cmap = cm.Greys_r, interpolation = 'none')    \n",
    "plt.show()\n",
    "print('Predicted label: ' + str(label_names[np.argmax(model.predict(test_features)[0])]))\n",
    "print('Actual label: ' + str(label_names[np.argmax(test_labels[0])]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
